**é‡åŒ–åŠåŠ é€Ÿ**
è¿™æ˜¯ä¸€ä¸ªåŸºäºStable Diffusion 3çš„3D Transformeræ¨¡å‹å®ç°ï¼Œä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š <br/>
 <br/>
1. æ¨¡å‹æ¶æ„ï¼š

ä½¿ç”¨äº†SD3Transformer3DModelï¼Œç»§æ‰¿è‡ªSD3Transformer2DModel <br/>
åŒ…å«å¤§é‡çš„attentionå±‚ï¼ˆ38ä¸ªattention headsï¼‰ <br/>
ä½¿ç”¨äº†patch embeddingå’Œposition embedding <br/>
æ”¯æŒNPUï¼ˆæ˜‡è…¾ï¼‰åŠ é€Ÿ <br/> <br/>

2. æ˜¾å­˜å ç”¨å¤§çš„ä¸»è¦åŸå› ï¼š
 <br/>
å¤§é‡çš„attentionå±‚ï¼ˆnum_layers=38, num_attention_heads=38ï¼‰ <br/>
è¾ƒå¤§çš„attention headç»´åº¦ï¼ˆattention_head_dim=64ï¼‰ <br/>
3Dæ•°æ®å¤„ç†ï¼ˆè§†é¢‘æˆ–3Dæ•°æ®ï¼‰éœ€è¦æ›´å¤šæ˜¾å­˜ <br/>
QKVçŸ©é˜µè®¡ç®—å’Œå­˜å‚¨ <br/>
 <br/>
**æ˜¾å­˜å ç”¨å¤§çš„ä¸»è¦åŸå› åŒ…æ‹¬ï¼š** <br/>
 <br/>
1. æ¨¡å‹ç»„æˆï¼š <br/>
 <br/>
VAEç¼–ç å™¨/è§£ç å™¨ <br/>
å¤šä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼ˆCLIPå’ŒT5ï¼‰ <br/>
3D Transformeræ¨¡å‹ <br/>
å¤„ç†è§†é¢‘æ•°æ®ï¼ˆå¤šå¸§ï¼‰ <br/>
 <br/>
2. è¿è¡Œæ—¶æ˜¾å­˜å ç”¨ï¼š <br/>
 <br/>
å¤§é‡ä¸­é—´çŠ¶æ€å’Œæ¢¯åº¦å­˜å‚¨ <br/>
å¤šä¸ªæ‰¹æ¬¡çš„è§†é¢‘å¸§å¤„ç† <br/>
ç”Ÿæˆè¿‡ç¨‹ä¸­çš„latentç©ºé—´è®¡ç®— <br/>
Classifier-Free Guidanceéœ€è¦ä¸¤æ¬¡å‰å‘ä¼ æ’­ <br/>
 <br/>
**ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨çš„æ–¹æ¡ˆï¼š**
 <br/>
1. å®æ—¶é‡åŒ–æ–¹æ¡ˆï¼š <br/>
 <br/>
å¯¹Transformerå’Œæ–‡æœ¬ç¼–ç å™¨è¿›è¡ŒINT8é‡åŒ– <br/>
ä½¿ç”¨åŠ¨æ€é‡åŒ–æŠ€æœ¯ï¼Œåªåœ¨æ¨ç†æ—¶è¿›è¡Œé‡åŒ– <br/>
å¯ä»¥ä½¿ç”¨torch.quantizationæˆ–bitsandbytesåº“ <br/>
 <br/>
2. æ¨¡å‹å¸è½½æ–¹æ¡ˆï¼š <br/>
 <br/>
ä½¿ç”¨CPU offloadingï¼Œå°†ä¸éœ€è¦çš„æ¨¡å‹ç»„ä»¶ä¸´æ—¶ç§»åˆ°CPU <br/>
å®ç°æ¸è¿›å¼åŠ è½½ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹ç»„ä»¶ <br/>
ä½¿ç”¨accelerateåº“çš„device_mapåŠŸèƒ½ <br/>
 <br/>
**æ˜¾å­˜å ç”¨å¤§ä¸»è¦æ˜¯å› ä¸ºï¼š**
 <br/>
1. å¤šä¸ªå¤§å‹æ¨¡å‹ç»„ä»¶åŒæ—¶åŠ è½½ <br/>
2. è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¤§é‡ä¸­é—´çŠ¶æ€ <br/>
3. Classifier-Free Guidanceéœ€è¦ä¸¤æ¬¡å‰å‘ä¼ æ’­ <br/>
4. å¤„ç†è§†é¢‘æ•°æ®éœ€è¦æ›´å¤šæ˜¾å­˜ <br/>
  <br/>
**ä¼˜åŒ–æ–¹æ¡ˆï¼š**
 <br/>
1. æ·»åŠ æ¨¡å‹å¸è½½åŠŸèƒ½ <br/>
2. å®ç°åŠ¨æ€é‡åŒ– <br/>
3. ä¼˜åŒ–å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„æ˜¾å­˜ä½¿ç”¨ <br/>
 <br/>
**æ·»åŠ ä»¥ä¸‹åŠŸèƒ½ï¼š**
 <br/>
1. æ·»åŠ ä¸€ä¸ªenable_model_cpu_offloadæ–¹æ³•ï¼Œç”¨äºå°†æ¨¡å‹ç»„ä»¶å¸è½½åˆ°CPU <br/>
2. æ·»åŠ ä¸€ä¸ªenable_sequential_cpu_offloadæ–¹æ³•ï¼Œç”¨äºæŒ‰é¡ºåºå¸è½½æ¨¡å‹ç»„ä»¶ <br/>
3. æ·»åŠ ä¸€ä¸ªenable_model_quantizationæ–¹æ³•ï¼Œç”¨äºé‡åŒ–æ¨¡å‹ <br/>
4. ä¿®æ”¹__call__æ–¹æ³•ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ <br/>
 <br/>


---
# ContentV: Efficient Training of Video Generation Models with Limited Compute

<div align="center">
<p align="center">
  <a href="https://contentv.github.io">
    <img
      src="https://img.shields.io/badge/Demo-Project Page-0A66C2?logo=googlechrome&logoColor=blue"
      alt="Project Page"
    />
  </a>
  <a href='https://arxiv.org/abs/2506.05343'>
    <img
      src="https://img.shields.io/badge/Tech Report-ArXiv-red?logo=arxiv&logoColor=red"
      alt="Tech Report"
    />
  </a>
  <a href="https://huggingface.co/ByteDance/ContentV-8B">
    <img 
        src="https://img.shields.io/badge/HuggingFace-Model-yellow?logo=huggingface&logoColor=yellow" 
        alt="Model"
    />
  </a>
  <a href="https://github.com/bytedance/ContentV">
    <img 
        src="https://img.shields.io/badge/Code-GitHub-orange?logo=github&logoColor=white" 
        alt="Code"
    />
  </a>
  <a href="https://www.apache.org/licenses/LICENSE-2.0">
    <img
      src="https://img.shields.io/badge/License-Apache 2.0-5865F2?logo=apache&logoColor=purple"
      alt="License"
    />
  </a>
</p>
</div>

This project presents *ContentV*, an efficient framework for accelerating the training of DiT-based video generation models through three key innovations:

- A minimalist architecture that maximizes reuse of pre-trained image generation models for video synthesis
- A systematic multi-stage training strategy leveraging flow matching for enhanced efficiency
- A cost-effective reinforcement learning with human feedback framework that improves generation quality without requiring additional human annotations

Our open-source 8B model (based on Stable Diffusion 3.5 Large and Wan-VAE) achieves state-of-the-art result (85.14 on VBench) in only 4 weeks of training with 256Ã—64GB NPUs.

<div align="center">
    <img src="./assets/demo.jpg" width="100%">
    <img src="./assets/arch.png" width="100%">
</div>

## âš¡ Quickstart

#### Recommended PyTorch Version

- GPU: torch >= 2.3.1 (CUDA >= 12.2)
- NPU: torch and torch-npu >= 2.1.0 (CANN >= 8.0.RC2). Please refer to [Ascend Extension for PyTorch](https://gitee.com/ascend/pytorch) for the installation of torch-npu.

#### Installation

```bash
git clone https://github.com/bytedance/ContentV.git
cd ContentV
pip3 install -r requirements.txt
```

#### T2V Generation

```bash
## For GPU
python3 demo.py
## For NPU
USE_ASCEND_NPU=1 python3 demo.py
```

## ğŸ“Š VBench

| Model | Total Score | Quality Score | Semantic Score | Human Action | Scene | Dynamic Degree | Multiple Objects  | Appear. Style |
|----------------------|--------|-------|-------|-------|-------|-------|-------|-------|
| Wan2.1-14B           | 86.22  | 86.67 | 84.44 | 99.20 | 61.24 | 94.26 | 86.59 | 21.59 |
| **ContentV (Long)**  | 85.14  | 86.64 | 79.12 | 96.80 | 57.38 | 83.05 | 71.41 | 23.02 |
| Gokuâ€                 | 84.85  | 85.60 | 81.87 | 97.60 | 57.08 | 76.11 | 79.48 | 23.08 |
| Open-Sora 2.0        | 84.34  | 85.40 | 80.12 | 95.40 | 52.71 | 71.39 | 77.72 | 22.98 |
| Soraâ€                 | 84.28  | 85.51 | 79.35 | 98.20 | 56.95 | 79.91 | 70.85 | 24.76 |
| **ContentV (Short)** | 84.11  | 86.23 | 75.61 | 89.60 | 44.02 | 79.26 | 74.58 | 21.21 |
| EasyAnimate 5.1      | 83.42  | 85.03 | 77.01 | 95.60 | 54.31 | 57.15 | 66.85 | 23.06 |
| Kling 1.6â€            | 83.40  | 85.00 | 76.99 | 96.20 | 55.57 | 62.22 | 63.99 | 20.75 |
| HunyuanVideo         | 83.24  | 85.09 | 75.82 | 94.40 | 53.88 | 70.83 | 68.55 | 19.80 |
| CogVideoX-5B         | 81.61  | 82.75 | 77.04 | 99.40 | 53.20 | 70.97 | 62.11 | 24.91 |
| Pika-1.0â€             | 80.69  | 82.92 | 71.77 | 86.20 | 49.83 | 47.50 | 43.08 | 22.26 |
| VideoCrafter-2.0     | 80.44  | 82.20 | 73.42 | 95.00 | 55.29 | 42.50 | 40.66 | 25.13 |
| AnimateDiff-V2       | 80.27  | 82.90 | 69.75 | 92.60 | 50.19 | 40.83 | 36.88 | 22.42 |
| OpenSora 1.2         | 79.23  | 80.71 | 73.30 | 85.80 | 42.47 | 47.22 | 58.41 | 23.89 |

## âœ… Todo List
- [x] Inference code and checkpoints
- [ ] Training code of RLHF

## ğŸ§¾ License
This code repository and part of the model weights are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). Please note that:
- MM DiT are derived from [Stable Diffusion 3.5 Large](https://huggingface.co/stabilityai/stable-diffusion-3.5-large) and trained with video samples. This Stability AI Model is licensed under the [Stability AI Community License](https://stability.ai/community-license-agreement), Copyright Â©  Stability AI Ltd. All Rights Reserved
- Video VAE from [Wan2.1](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B) is licensed under [Apache 2.0 License](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/blob/main/LICENSE.txt)

## â¤ï¸ Acknowledgement
* [Stable Diffusion 3.5 Large](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)
* [Wan2.1](https://github.com/Wan-Video/Wan2.1)
* [Diffusers](https://github.com/huggingface/diffusers)
* [HuggingFace](https://huggingface.co)

## ğŸ”— Citation

```bibtex
@article{contentv2025,
  title     = {ContentV: Efficient Training of Video Generation Models with Limited Compute},
  author    = {Bytedance Douyin Content Team},
  journal   = {arXiv preprint arXiv:2506.05343},
  year      = {2025}
  }
```
