![image](https://github.com/user-attachments/assets/4c6dc3a2-e435-44dd-87a1-58facdc2108e)
<br/>
å¯ç”¨æ¨¡å‹CPUå¸è½½ï¼Œå°†æ˜¾å­˜å ç”¨é™ä½è‡³æ°‘ç”¨çº§24Gæ˜¾å¡å¯ç”¨ã€‚ï¼ˆæš‚æœªå®ç°æ¨¡å‹é‡åŒ–åŠé¡ºåºå¸è½½ï¼‰ <br/>
Clone ä»“åº“ï¼ŒæŒ‰ç…§å®˜æ–¹æ­¥éª¤å»ºç«‹è™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–ï¼Œä¸‹è½½å®˜æ–¹ä»“åº“æ¨¡å‹ <br/>
å¯åŠ¨ï¼špython app.py<br/>
 <br/>
![image](https://github.com/user-attachments/assets/4171189e-d9f1-4e62-87d6-df8012417083)
<br/>
 <br/>
### ä½¿ç”¨è¯´æ˜
æ¨¡å‹è·¯å¾„: è¾“å…¥å·²ä¸‹è½½çš„ContentVæ¨¡å‹è·¯å¾„<br/>
æç¤ºè¯è®¾ç½®:<br/>
æ­£é¢æç¤ºè¯ï¼šæè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹<br/>
è´Ÿé¢æç¤ºè¯ï¼šæè¿°æ‚¨ä¸æƒ³åœ¨è§†é¢‘ä¸­å‡ºç°çš„å†…å®¹<br/>
è§†é¢‘å‚æ•°:<br/>
æ€»å¸§æ•°ï¼šè§†é¢‘çš„æ€»å¸§æ•°ï¼ˆå½±å“ç”Ÿæˆæ—¶é—´ï¼‰<br/>
å¸§ç‡ï¼šè§†é¢‘çš„æ’­æ”¾é€Ÿåº¦<br/>
åˆ†è¾¨ç‡ï¼šè§†é¢‘çš„å®½åº¦å’Œé«˜åº¦<br/>
æ¨ç†æ­¥æ•°ï¼šç”Ÿæˆè´¨é‡ï¼ˆæ­¥æ•°è¶Šå¤šè´¨é‡è¶Šå¥½ï¼Œä½†è€—æ—¶æ›´é•¿ï¼‰<br/>
å†…å­˜ä¼˜åŒ–é€‰é¡¹:<br/>
é¡ºåºCPUå¸è½½ï¼šå°†ä¸æ´»è·ƒçš„æ¨¡å‹ç»„ä»¶é€ä¸ªå¸è½½åˆ°CPUï¼Œé€‚ç”¨äºæ˜¾å­˜è¾ƒå°çš„GPU<br/>
æ¨¡å‹CPUå¸è½½ï¼šæ•´ä¸ªæ¨¡å‹çº§åˆ«çš„CPUå¸è½½ï¼Œæ¯”é¡ºåºå¸è½½æ€§èƒ½æ›´å¥½<br/>
VAEåˆ‡ç‰‡ï¼šå°†VAEè¾“å…¥åˆ†ç‰‡å¤„ç†ï¼Œå‡å°‘æ˜¾å­˜ä½¿ç”¨<br/>
VAEå¹³é“ºï¼šå°†VAEè¾“å…¥å¹³é“ºå¤„ç†ï¼Œå¯ä»¥å¤„ç†æ›´å¤§çš„å›¾åƒ<br/>
æ³¨æ„åŠ›åˆ‡ç‰‡ï¼šå°†æ³¨æ„åŠ›è®¡ç®—åˆ†ç‰‡å¤„ç†ï¼Œå‡å°‘æ˜¾å­˜ä½¿ç”¨ä½†å¯èƒ½ç•¥å¾®é™ä½é€Ÿåº¦<br/>
<br/>

### æ³¨æ„ï¼š
<br/>
é¡ºåºCPUå¸è½½å’Œæ¨¡å‹CPUå¸è½½åªèƒ½é€‰æ‹©å…¶ä¸­ä¸€ä¸ª<br/>
å…¶ä»–ä¼˜åŒ–é€‰é¡¹å¯ä»¥ç»„åˆä½¿ç”¨<br/>
ç”Ÿæˆè§†é¢‘å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…<br/>
ç”Ÿæˆçš„è§†é¢‘å°†ä¿å­˜åœ¨outputsç›®å½•ä¸‹<br/>
<br/>

### ä¼˜åŒ–æ•ˆæœ

æ˜¾å­˜ä½¿ç”¨é™ä½ï¼šæ ¹æ®é…ç½®å’Œä½¿ç”¨åœºæ™¯ï¼Œå¯ä»¥é™ä½30-70%çš„æ˜¾å­˜ä½¿ç”¨ <br/>
å†…å­˜å ç”¨æ›´å‡è¡¡ï¼šé€šè¿‡CPUå¸è½½å®ç°GPUå’ŒCPUä¹‹é—´çš„å†…å­˜å¹³è¡¡ <br/>
æ”¯æŒæ›´å¤§åˆ†è¾¨ç‡ï¼šé€šè¿‡ä¼˜åŒ–å¯ä»¥å¤„ç†æ›´é«˜åˆ†è¾¨ç‡çš„è§†é¢‘ç”Ÿæˆ <br/>
æ‰¹å¤„ç†èƒ½åŠ›æå‡ï¼šåœ¨ç›¸åŒæ˜¾å­˜ä¸‹å¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡ <br/> 
### æ³¨æ„äº‹é¡¹
 <br/> 
1. CPUå¸è½½ä¼šç•¥å¾®å¢åŠ å¤„ç†æ—¶é—´ï¼Œå»ºè®®æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©æ˜¯å¦å¯ç”¨ <br/>

2. æ¨¡å‹é‡åŒ–å¯èƒ½ä¼šè½»å¾®å½±å“ç”Ÿæˆè´¨é‡ï¼Œå¦‚æœè¿½æ±‚æœ€é«˜è´¨é‡å¯ä»¥åªå¯ç”¨CPUå¸è½½ <br/>

3. å»ºè®®åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡è§†é¢‘æˆ–é•¿åºåˆ—æ—¶å¯ç”¨æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ <br/>

4. å¯ä»¥é€šè¿‡ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µæ¥è°ƒæ•´ä¼˜åŒ–ç­–ç•¥ <br/>
 
### å»ºè®®é…ç½®
 <br/>
å¯¹äº24GBæ˜¾å­˜çš„GPUï¼šå¯ä»¥åªå¯ç”¨CPUå¸è½½ <br/>
å¯¹äº16GBæ˜¾å­˜çš„GPUï¼šå»ºè®®åŒæ—¶å¯ç”¨CPUå¸è½½å’Œæ¨¡å‹é‡åŒ– <br/>
å¯¹äº16GBåŠä»¥ä¸‹æ˜¾å­˜çš„GPUï¼šå¿…é¡»å¯ç”¨æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ <br/>
é€šè¿‡åˆç†ä½¿ç”¨è¿™äº›ä¼˜åŒ–åŠŸèƒ½ï¼Œå¯ä»¥åœ¨æœ‰é™çš„æ˜¾å­˜æ¡ä»¶ä¸‹å®ç°æ›´å¥½çš„è§†é¢‘ç”Ÿæˆæ•ˆæœã€‚ <br/>

### Coderå·¥ä½œè®°å½•
 <br/>
 
**é‡åŒ–åŠåŠ é€Ÿ**
è¿™æ˜¯ä¸€ä¸ªåŸºäºStable Diffusion 3çš„3D Transformeræ¨¡å‹å®ç°ï¼Œä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š <br/>
 <br/>
1. æ¨¡å‹æ¶æ„ï¼š

ä½¿ç”¨äº†SD3Transformer3DModelï¼Œç»§æ‰¿è‡ªSD3Transformer2DModel <br/>
åŒ…å«å¤§é‡çš„attentionå±‚ï¼ˆ38ä¸ªattention headsï¼‰ <br/>
ä½¿ç”¨äº†patch embeddingå’Œposition embedding <br/>
æ”¯æŒNPUï¼ˆæ˜‡è…¾ï¼‰åŠ é€Ÿ <br/> <br/>

2. æ˜¾å­˜å ç”¨å¤§çš„ä¸»è¦åŸå› ï¼š
 <br/>
å¤§é‡çš„attentionå±‚ï¼ˆnum_layers=38, num_attention_heads=38ï¼‰ <br/>
è¾ƒå¤§çš„attention headç»´åº¦ï¼ˆattention_head_dim=64ï¼‰ <br/>
3Dæ•°æ®å¤„ç†ï¼ˆè§†é¢‘æˆ–3Dæ•°æ®ï¼‰éœ€è¦æ›´å¤šæ˜¾å­˜ <br/>
QKVçŸ©é˜µè®¡ç®—å’Œå­˜å‚¨ <br/>
 <br/>
**æ˜¾å­˜å ç”¨å¤§çš„ä¸»è¦åŸå› åŒ…æ‹¬ï¼š** <br/>
 <br/>
1. æ¨¡å‹ç»„æˆï¼š <br/>
 <br/>
VAEç¼–ç å™¨/è§£ç å™¨ <br/>
å¤šä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼ˆCLIPå’ŒT5ï¼‰ <br/>
3D Transformeræ¨¡å‹ <br/>
å¤„ç†è§†é¢‘æ•°æ®ï¼ˆå¤šå¸§ï¼‰ <br/>
 <br/>
2. è¿è¡Œæ—¶æ˜¾å­˜å ç”¨ï¼š <br/>
 <br/>
å¤§é‡ä¸­é—´çŠ¶æ€å’Œæ¢¯åº¦å­˜å‚¨ <br/>
å¤šä¸ªæ‰¹æ¬¡çš„è§†é¢‘å¸§å¤„ç† <br/>
ç”Ÿæˆè¿‡ç¨‹ä¸­çš„latentç©ºé—´è®¡ç®— <br/>
Classifier-Free Guidanceéœ€è¦ä¸¤æ¬¡å‰å‘ä¼ æ’­ <br/>
 <br/>
**ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨çš„æ–¹æ¡ˆï¼š**
 <br/>
1. å®æ—¶é‡åŒ–æ–¹æ¡ˆï¼š <br/>
 <br/>
å¯¹Transformerå’Œæ–‡æœ¬ç¼–ç å™¨è¿›è¡ŒINT8é‡åŒ– <br/>
ä½¿ç”¨åŠ¨æ€é‡åŒ–æŠ€æœ¯ï¼Œåªåœ¨æ¨ç†æ—¶è¿›è¡Œé‡åŒ– <br/>
å¯ä»¥ä½¿ç”¨torch.quantizationæˆ–bitsandbytesåº“ <br/>
 <br/>
2. æ¨¡å‹å¸è½½æ–¹æ¡ˆï¼š <br/>
 <br/>
ä½¿ç”¨CPU offloadingï¼Œå°†ä¸éœ€è¦çš„æ¨¡å‹ç»„ä»¶ä¸´æ—¶ç§»åˆ°CPU <br/>
å®ç°æ¸è¿›å¼åŠ è½½ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹ç»„ä»¶ <br/>
ä½¿ç”¨accelerateåº“çš„device_mapåŠŸèƒ½ <br/>
 <br/>
**æ˜¾å­˜å ç”¨å¤§ä¸»è¦æ˜¯å› ä¸ºï¼š**
 <br/>
1. å¤šä¸ªå¤§å‹æ¨¡å‹ç»„ä»¶åŒæ—¶åŠ è½½ <br/>
2. è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¤§é‡ä¸­é—´çŠ¶æ€ <br/>
3. Classifier-Free Guidanceéœ€è¦ä¸¤æ¬¡å‰å‘ä¼ æ’­ <br/>
4. å¤„ç†è§†é¢‘æ•°æ®éœ€è¦æ›´å¤šæ˜¾å­˜ <br/>
  <br/>
**ä¼˜åŒ–æ–¹æ¡ˆï¼š**
 <br/>
1. æ·»åŠ æ¨¡å‹å¸è½½åŠŸèƒ½ <br/>
2. å®ç°åŠ¨æ€é‡åŒ– <br/>
3. ä¼˜åŒ–å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„æ˜¾å­˜ä½¿ç”¨ <br/>
 <br/>
**æ·»åŠ ä»¥ä¸‹åŠŸèƒ½ï¼š**
 <br/>
1. æ·»åŠ ä¸€ä¸ªenable_model_cpu_offloadæ–¹æ³•ï¼Œç”¨äºå°†æ¨¡å‹ç»„ä»¶å¸è½½åˆ°CPU <br/>
2. æ·»åŠ ä¸€ä¸ªenable_sequential_cpu_offloadæ–¹æ³•ï¼Œç”¨äºæŒ‰é¡ºåºå¸è½½æ¨¡å‹ç»„ä»¶ <br/>
3. æ·»åŠ ä¸€ä¸ªenable_model_quantizationæ–¹æ³•ï¼Œç”¨äºé‡åŒ–æ¨¡å‹ <br/>
4. ä¿®æ”¹__call__æ–¹æ³•ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ <br/>
 <br/>

 
 
### ä»¥ä¸‹å†…å®¹æ¥è‡ªåŸå§‹ä»£ç ä»“åº“


---
# ContentV: Efficient Training of Video Generation Models with Limited Compute

<div align="center">
<p align="center">
  <a href="https://contentv.github.io">
    <img
      src="https://img.shields.io/badge/Demo-Project Page-0A66C2?logo=googlechrome&logoColor=blue"
      alt="Project Page"
    />
  </a>
  <a href='https://arxiv.org/abs/2506.05343'>
    <img
      src="https://img.shields.io/badge/Tech Report-ArXiv-red?logo=arxiv&logoColor=red"
      alt="Tech Report"
    />
  </a>
  <a href="https://huggingface.co/ByteDance/ContentV-8B">
    <img 
        src="https://img.shields.io/badge/HuggingFace-Model-yellow?logo=huggingface&logoColor=yellow" 
        alt="Model"
    />
  </a>
  <a href="https://github.com/bytedance/ContentV">
    <img 
        src="https://img.shields.io/badge/Code-GitHub-orange?logo=github&logoColor=white" 
        alt="Code"
    />
  </a>
  <a href="https://www.apache.org/licenses/LICENSE-2.0">
    <img
      src="https://img.shields.io/badge/License-Apache 2.0-5865F2?logo=apache&logoColor=purple"
      alt="License"
    />
  </a>
</p>
</div>

This project presents *ContentV*, an efficient framework for accelerating the training of DiT-based video generation models through three key innovations:

- A minimalist architecture that maximizes reuse of pre-trained image generation models for video synthesis
- A systematic multi-stage training strategy leveraging flow matching for enhanced efficiency
- A cost-effective reinforcement learning with human feedback framework that improves generation quality without requiring additional human annotations

Our open-source 8B model (based on Stable Diffusion 3.5 Large and Wan-VAE) achieves state-of-the-art result (85.14 on VBench) in only 4 weeks of training with 256Ã—64GB NPUs.

<div align="center">
    <img src="./assets/demo.jpg" width="100%">
    <img src="./assets/arch.png" width="100%">
</div>

## âš¡ Quickstart

#### Recommended PyTorch Version

- GPU: torch >= 2.3.1 (CUDA >= 12.2)
- NPU: torch and torch-npu >= 2.1.0 (CANN >= 8.0.RC2). Please refer to [Ascend Extension for PyTorch](https://gitee.com/ascend/pytorch) for the installation of torch-npu.

#### Installation

```bash
git clone https://github.com/bytedance/ContentV.git
cd ContentV
pip3 install -r requirements.txt
```

#### T2V Generation

```bash
## For GPU
python3 demo.py
## For NPU
USE_ASCEND_NPU=1 python3 demo.py
```

## ğŸ“Š VBench

| Model | Total Score | Quality Score | Semantic Score | Human Action | Scene | Dynamic Degree | Multiple Objects  | Appear. Style |
|----------------------|--------|-------|-------|-------|-------|-------|-------|-------|
| Wan2.1-14B           | 86.22  | 86.67 | 84.44 | 99.20 | 61.24 | 94.26 | 86.59 | 21.59 |
| **ContentV (Long)**  | 85.14  | 86.64 | 79.12 | 96.80 | 57.38 | 83.05 | 71.41 | 23.02 |
| Gokuâ€                 | 84.85  | 85.60 | 81.87 | 97.60 | 57.08 | 76.11 | 79.48 | 23.08 |
| Open-Sora 2.0        | 84.34  | 85.40 | 80.12 | 95.40 | 52.71 | 71.39 | 77.72 | 22.98 |
| Soraâ€                 | 84.28  | 85.51 | 79.35 | 98.20 | 56.95 | 79.91 | 70.85 | 24.76 |
| **ContentV (Short)** | 84.11  | 86.23 | 75.61 | 89.60 | 44.02 | 79.26 | 74.58 | 21.21 |
| EasyAnimate 5.1      | 83.42  | 85.03 | 77.01 | 95.60 | 54.31 | 57.15 | 66.85 | 23.06 |
| Kling 1.6â€            | 83.40  | 85.00 | 76.99 | 96.20 | 55.57 | 62.22 | 63.99 | 20.75 |
| HunyuanVideo         | 83.24  | 85.09 | 75.82 | 94.40 | 53.88 | 70.83 | 68.55 | 19.80 |
| CogVideoX-5B         | 81.61  | 82.75 | 77.04 | 99.40 | 53.20 | 70.97 | 62.11 | 24.91 |
| Pika-1.0â€             | 80.69  | 82.92 | 71.77 | 86.20 | 49.83 | 47.50 | 43.08 | 22.26 |
| VideoCrafter-2.0     | 80.44  | 82.20 | 73.42 | 95.00 | 55.29 | 42.50 | 40.66 | 25.13 |
| AnimateDiff-V2       | 80.27  | 82.90 | 69.75 | 92.60 | 50.19 | 40.83 | 36.88 | 22.42 |
| OpenSora 1.2         | 79.23  | 80.71 | 73.30 | 85.80 | 42.47 | 47.22 | 58.41 | 23.89 |

## âœ… Todo List
- [x] Inference code and checkpoints
- [ ] Training code of RLHF

## ğŸ§¾ License
This code repository and part of the model weights are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). Please note that:
- MM DiT are derived from [Stable Diffusion 3.5 Large](https://huggingface.co/stabilityai/stable-diffusion-3.5-large) and trained with video samples. This Stability AI Model is licensed under the [Stability AI Community License](https://stability.ai/community-license-agreement), Copyright Â©  Stability AI Ltd. All Rights Reserved
- Video VAE from [Wan2.1](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B) is licensed under [Apache 2.0 License](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/blob/main/LICENSE.txt)

## â¤ï¸ Acknowledgement
* [Stable Diffusion 3.5 Large](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)
* [Wan2.1](https://github.com/Wan-Video/Wan2.1)
* [Diffusers](https://github.com/huggingface/diffusers)
* [HuggingFace](https://huggingface.co)

## ğŸ”— Citation

```bibtex
@article{contentv2025,
  title     = {ContentV: Efficient Training of Video Generation Models with Limited Compute},
  author    = {Bytedance Douyin Content Team},
  journal   = {arXiv preprint arXiv:2506.05343},
  year      = {2025}
  }
```
